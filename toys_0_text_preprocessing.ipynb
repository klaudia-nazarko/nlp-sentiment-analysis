{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klaud\\Anaconda3\\envs\\fastai-nlp\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import scipy\n",
    "\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk import stem\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word -> text -> text_corpus\n",
    "token -> text_token -> tokens_corpus\n",
    "ind -> text_ind -> ind_corpus\n",
    "corpus_counter\n",
    "\n",
    "text: string ze słowami -> word\n",
    "\n",
    "text_token: lista z tokenami -> token\n",
    "\n",
    "text_ind: lista z indexami tokenów -> ind\n",
    "\n",
    "text_corpus: lista stringów ze słowami\n",
    "\n",
    "tokens_corpus: lista list z tokenami\n",
    "\n",
    "corpus_counter: słownik ze słowami i częstotliwościami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def remove_stop_words(text_token):\n",
    "    return [token for token in text_token if token not in stop_words.ENGLISH_STOP_WORDS]\n",
    "\n",
    "def lem_words(text_token):\n",
    "    lem = stem.WordNetLemmatizer()\n",
    "    return [lem.lemmatize(token, pos='v') for token in text_token]\n",
    "\n",
    "def distinct_corpus_words(tokens_corpus):\n",
    "    tokens_corpus_flatten = [token for tokens in tokens_corpus for token in tokens]\n",
    "    corpus_counter = Counter(tokens_corpus_flatten).most_common()\n",
    "    return corpus_counter, len(corpus_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_single_text(text, translation_table=translation_table):\n",
    "    text = str(text).lower()\n",
    "    text = text.translate(translation_table)\n",
    "\n",
    "    text_tokens = nltk.word_tokenize(text)\n",
    "    text_tokens = remove_stop_words(text_tokens)\n",
    "    text_tokens = lem_words(text_tokens)\n",
    "    \n",
    "    return text_tokens\n",
    "\n",
    "def normalize_text(text_corpus, translation_table=translation_table):\n",
    "    return [normalize_single_text(text, translation_table=translation_table) for text in text_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(corpus_counter):\n",
    "    word2ind = {corpus_counter[i][0]: i for i in range(len(corpus_counter))}\n",
    "    ind2word = [word[0] for word in corpus_counter]\n",
    "    return word2ind, ind2word\n",
    "\n",
    "def text_token2ind(text_token, word2ind):\n",
    "    return [word2ind[token] for token in text_token if token in word2ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bow(ind_corpus, n_tokens, max_features=None):\n",
    "    \"\"\"Max features takes n features with the lowest index - assumes that lower index -> higher number of occurrences\"\"\"\n",
    "    \n",
    "    if max_features:\n",
    "        ind_corpus = [[ind for ind in text_ind if ind < max_features] for text_ind in ind_corpus]\n",
    "        n_tokens = max_features\n",
    "\n",
    "    values = []\n",
    "    col_ind = []\n",
    "    row_pointer = [0]\n",
    "\n",
    "    for features in ind_corpus:\n",
    "        feature_counter = Counter(features)\n",
    "        col_ind.extend(feature_counter.keys())\n",
    "        values.extend(feature_counter.values())\n",
    "        row_pointer.append(len(values))\n",
    "\n",
    "    S = scipy.sparse.csr_matrix((values, col_ind, row_pointer),\n",
    "                                       shape=(len(row_pointer) - 1, n_tokens),\n",
    "                                       dtype=int)\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_bow(text_ind, n_tokens, max_features=None):\n",
    "    if max_features:\n",
    "        text_ind = [ind for ind in text_ind if ind < max_features]\n",
    "        n_tokens = max_features\n",
    "\n",
    "    single_bow = np.zeros(n_tokens)\n",
    "    for ind in text_ind:\n",
    "        single_bow[ind] += 1\n",
    "    \n",
    "    return single_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_co_occurrence_matrix(ind_corpus, n_tokens, window_size=4):\n",
    "    row_ind = []\n",
    "    col_ind = []\n",
    "    values = []\n",
    "\n",
    "    for text_ind in ind_corpus:\n",
    "        for i in range(len(text_ind)):\n",
    "            for j in range(max(i-window_size, 0), min(i + window_size + 1, len(text_ind))):\n",
    "                if i != j:\n",
    "                    row_ind.extend([text_ind[i]])\n",
    "                    col_ind.extend([text_ind[j]])\n",
    "                    values.extend([1])\n",
    "    \n",
    "    S = scipy.sparse.coo_matrix((values, (row_ind, col_ind)), shape=(n_tokens, n_tokens))\n",
    "\n",
    "    return S\n",
    "\n",
    "def matrix_reduce(M, method, n_dim=2, n_iter=10):\n",
    "    \n",
    "    try:\n",
    "        if method == 'svd':\n",
    "            decomposition = TruncatedSVD(n_components=n_dim, n_iter=n_iter)\n",
    "        elif method == 'nmf':\n",
    "            decomposition = NMF(n_components=n_dim)\n",
    "        \n",
    "        M_reduced = decomposition.fit_transform(M)\n",
    "        return M_reduced\n",
    "        \n",
    "    except UnboundLocalError:\n",
    "        print('Choose either svd or nmf method')\n",
    "\n",
    "def avg_svd_embeddings(text_ind, reduced_co_occurrence_matrix, word2ind):\n",
    "    i = len(text_ind)\n",
    "\n",
    "    if i>=1:\n",
    "        return sum([reduced_co_occurrence_matrix[ind] for ind in text_ind])/i\n",
    "    return np.zeros(reduced_co_occurrence_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_w2v_embeddings(text_token, w2v_model):\n",
    "    words = [token for token in text_token if token in w2v_model.wv.vocab]\n",
    "    if len(words)>=1:\n",
    "        return np.mean(w2v_model.wv[words], axis=0)\n",
    "    return np.zeros(w2v_model.trainables.layer1_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "Creating word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>737813</th>\n",
       "      <td>Another great doll added to our collection. Du...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321082</th>\n",
       "      <td>As good as old.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232672</th>\n",
       "      <td>This product works great on my RC car would de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118333</th>\n",
       "      <td>Love these. Put them away for x-mas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440936</th>\n",
       "      <td>Still loving it after two years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    review  sentiment\n",
       "737813   Another great doll added to our collection. Du...          1\n",
       "1321082                                    As good as old.          1\n",
       "1232672  This product works great on my RC car would de...          1\n",
       "1118333                Love these. Put them away for x-mas          1\n",
       "440936                     Still loving it after two years          1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/reviews_toys_games.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.914674872933972"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['sentiment'])/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews = df['review'].to_list()\n",
    "#sentiment = df['sentiment'].to_list()\n",
    "\n",
    "reviews = df['review'][:10000].to_list()\n",
    "sentiment = df['sentiment'][:10000].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_tokens = normalize_text(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in the dictionary:  236419\n",
      "Most common words:  [('love', 613117), ('great', 385104), ('play', 288507), ('toy', 242846), ('old', 228996), ('like', 218309), ('buy', 202374), ('kid', 196021), ('game', 193122), ('fun', 190516)]\n"
     ]
    }
   ],
   "source": [
    "corpus_counter, n_tokens = distinct_corpus_words(reviews_tokens)\n",
    "\n",
    "print('Words in the dictionary: ', n_tokens)\n",
    "print('Most common words: ', corpus_counter[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/reviews_tokens_10k.pickle', 'wb') as handle:\n",
    "    pickle.dump(reviews_tokens, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('data/sentiment_10k.pickle', 'wb') as handle:\n",
    "    pickle.dump(sentiment, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None\n",
    "reviews = None\n",
    "reviews_tokens = None\n",
    "sentiment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/reviews_tokens_10k.pickle', 'rb') as handle:\n",
    "    reviews_tokens = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_counter, n_tokens = distinct_corpus_words(reviews_tokens)\n",
    "word2ind, ind2word = build_dictionary(corpus_counter)\n",
    "reviews_ind = [text_token2ind(review, word2ind) for review in reviews_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_bow = build_bow(reviews_ind, n_tokens, 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1426227, 6000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text of test review:  ['ok', 'really', 'consider', 'book', 'really', 'small', 'disappoint']\n",
      "occurrences of words: \n",
      "book: 1 \n",
      "really: 2\n"
     ]
    }
   ],
   "source": [
    "ind_test = 15\n",
    "print('text of test review: ', reviews_tokens[ind_test])\n",
    "print('occurrences of words:',\n",
    "      '\\nbook:', S_bow.todense()[ind_test, word2ind['book']],\n",
    "      '\\nreally:', S_bow.todense()[ind_test, word2ind['really']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/M_bow_10k.pickle', 'wb') as handle:\n",
    "    pickle.dump(S_bow, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "S_bow = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert string to BOW vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_test = 'This game is amazing ^^, my son plays with it all the time! popolsku behavoir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game', 'amaze', 'son', 'play', 'time', 'popolsku', 'behavoir']\n",
      "[0, 221, 30, 1, 9, 7999]\n"
     ]
    }
   ],
   "source": [
    "review_tokens_test = normalize_single_text(review_test)\n",
    "print(review_tokens_test)\n",
    "\n",
    "review_ind_test = text_token2ind(review_tokens_test, word2ind)\n",
    "print(review_ind_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_single_bow(review_ind_test, n_tokens, 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "M_tfidf = vectorizer.fit_transform([' '.join(r) for r in reviews_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1426227, 236378)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text of test review:  ['ok', 'really', 'consider', 'book', 'really', 'small', 'disappoint']\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.45 TiB for an array with shape (1426227, 236378) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c020b93f4f1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text of test review: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreviews_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m print('TFIDF of words:',\n\u001b[1;32m----> 4\u001b[1;33m       \u001b[1;34m'\\nbook:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'book'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m       '\\nreally:', M_tfidf.todense()[ind_test, vectorizer.vocabulary_['really']])\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai-nlp\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36mtodense\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    849\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m         \"\"\"\n\u001b[1;32m--> 851\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai-nlp\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai-nlp\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.45 TiB for an array with shape (1426227, 236378) and data type float64"
     ]
    }
   ],
   "source": [
    "ind_test = 15\n",
    "print('text of test review: ', reviews_tokens[ind_test])\n",
    "print('TFIDF of words:',\n",
    "      '\\nbook:', M_tfidf.todense()[ind_test, vectorizer.vocabulary_['book']],\n",
    "      '\\nreally:', M_tfidf.todense()[ind_test, vectorizer.vocabulary_['really']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/M_tfidf_10k.pickle', 'wb') as handle:\n",
    "    pickle.dump(M_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "M_tfidf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert test string into TFIDF vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x13622 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([' '.join(review_tokens_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD & NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_co_occurrence = build_co_occurrence_matrix(reviews_ind, n_tokens, window_size=4)\n",
    "svd_reduced_co_occurrence = matrix_reduce(S_co_occurrence, method='svd', n_dim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_svd = np.stack([avg_svd_embeddings(ind, svd_reduced_co_occurrence, word2ind) for ind in reviews_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.14323424e+01, -3.47109825e+00, -2.67020404e+00, ...,\n",
       "        -1.88757093e+00,  6.84419604e-02,  4.51224859e+00],\n",
       "       [ 2.16992669e+02,  1.05707348e+01, -1.46835127e+01, ...,\n",
       "        -3.77584575e+00, -4.01213756e+00, -3.71439386e-01],\n",
       "       [ 4.55199337e+02, -3.17093833e+01, -8.54671822e+01, ...,\n",
       "         1.39537457e+01,  6.02229246e+00,  6.95342963e+00],\n",
       "       ...,\n",
       "       [ 2.63608525e+03, -1.08866951e+02,  2.31153097e+02, ...,\n",
       "         4.89310910e+01, -1.53355284e+01, -1.04527500e+01],\n",
       "       [ 6.57657544e+01, -6.06839347e+00, -9.53952804e+00, ...,\n",
       "        -8.10784544e+00, -5.53772244e-01,  5.31086296e+00],\n",
       "       [ 1.43775318e+02,  7.27239290e+01,  2.55859105e+01, ...,\n",
       "         3.76106324e+01,  2.49329050e+00, -1.01655368e+01]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1426227, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/M_svd_10k.pickle', 'wb') as handle:\n",
    "    pickle.dump(M_svd, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "M_svd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_reduced_co_occurrence = matrix_reduce(S_co_occurrence, method='nmf', n_dim=5)\n",
    "M_nmf = np.stack([avg_svd_embeddings(ind, nmf_reduced_co_occurrence, word2ind) for ind in reviews_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21164795,  0.03531371,  0.31712811, ...,  0.02142679,\n",
       "         0.07663577,  0.59293864],\n",
       "       [ 0.7533052 ,  0.36481111,  1.62439824, ...,  0.16128918,\n",
       "         0.473102  ,  1.15764214],\n",
       "       [ 1.09361196,  0.25399942,  5.83577518, ...,  0.36386534,\n",
       "         0.74307879,  2.78276504],\n",
       "       ...,\n",
       "       [10.79696704,  2.59788942, 17.33681545, ...,  8.44909389,\n",
       "         2.12239207,  1.49585677],\n",
       "       [ 0.42627795,  0.        ,  0.37320601, ...,  0.        ,\n",
       "         0.09127283,  1.41276781],\n",
       "       [ 0.        ,  0.7969728 ,  0.90182256, ...,  0.60171264,\n",
       "         0.37780882,  0.69107105]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1426227, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_nmf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/M_nmf_10k.pickle', 'wb') as handle:\n",
    "    pickle.dump(M_nmf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "M_nmf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert test string into SVD vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.77431155e+03, -5.75434644e+01,  8.09803398e+01, -1.17456111e+01,\n",
       "       -4.33351774e+01,  3.25419129e+01,  5.43330545e+01, -2.48050289e+01,\n",
       "        1.50935161e+00, -2.34834456e+01])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_svd_embeddings(review_ind_test, S_reduced_co_occurrence, word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=5,\n",
    "                     window=3,\n",
    "                     size=100,\n",
    "                     workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(reviews_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(625688996, 726801870)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(reviews_tokens, total_examples=w2v_model.corpus_count, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_word2vec = np.stack([avg_w2v_embeddings(review, w2v_model) for review in reviews_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1426227, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_word2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/M_word2vec_10k.pickle', 'wb') as handle:\n",
    "    pickle.dump(M_word2vec, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "M_word2vec = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert test string into SVD vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42165866, -0.19917676,  0.44199872,  0.6969125 ,  0.5052937 ,\n",
       "       -0.05001388,  0.23804888, -0.16238654,  0.75656104,  0.10875399,\n",
       "       -0.03664295,  0.22321317,  0.3091781 , -0.42105788,  0.6086213 ,\n",
       "       -0.31654102, -0.65119016,  0.45273328,  0.01955453,  0.03324833,\n",
       "        0.02535819, -0.10822463,  0.36700904, -0.3750741 , -0.27163497,\n",
       "        0.33239564,  0.04524777, -0.00577122, -0.08206176, -0.5412153 ,\n",
       "        0.09901704, -0.33308092,  0.09012656,  0.4178986 ,  0.2539987 ,\n",
       "       -0.6296872 ,  0.80539304, -0.12282996,  0.23620553, -0.19840494,\n",
       "       -0.03773532, -0.6750927 ,  0.24422427,  0.5969204 ,  0.41765147,\n",
       "        0.29813904,  0.59395635, -0.39825255, -0.27801678, -0.4181188 ,\n",
       "        0.13290724, -0.42702326,  0.03935174, -0.27779803,  0.11059706,\n",
       "        0.35624462, -0.4419004 ,  0.1847095 , -0.19821852,  0.1428484 ,\n",
       "       -0.49165335, -0.23037104, -0.01880818,  0.90063465,  0.27459517,\n",
       "        0.10313277,  0.06069721,  0.3767677 ,  0.16848317,  0.03099017,\n",
       "        0.16776259, -0.09641836, -0.05058963,  0.06431934,  0.32304725,\n",
       "        0.33333582, -0.46543503, -0.04799214, -0.09386587, -0.8075961 ,\n",
       "       -0.17889495,  1.0291219 ,  0.09229574, -0.45490837, -0.5865692 ,\n",
       "       -0.86364204,  0.08959617,  0.22101422, -0.26672754, -0.05881132,\n",
       "       -0.15876201, -0.979887  , -0.0159505 ,  0.1779228 ,  0.1880131 ,\n",
       "        0.2830646 , -0.5256817 ,  0.52423507, -0.30475622,  0.7345453 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_w2v_embeddings(review_tokens_test, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai-nlp",
   "language": "python",
   "name": "fastai-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
