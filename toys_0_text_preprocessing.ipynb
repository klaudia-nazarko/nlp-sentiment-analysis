{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klaud\\Anaconda3\\envs\\fastai-nlp\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word -> text -> text_corpus\n",
    "token -> text_token -> tokens_corpus\n",
    "ind -> text_ind -> ind_corpus\n",
    "corpus_counter\n",
    "\n",
    "text: string ze słowami -> word\n",
    "\n",
    "text_token: lista z tokenami -> token\n",
    "\n",
    "text_ind: lista z indexami tokenów -> ind\n",
    "\n",
    "text_corpus: lista stringów ze słowami\n",
    "\n",
    "tokens_corpus: lista list z tokenami\n",
    "\n",
    "corpus_counter: słownik ze słowami i częstotliwościami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def remove_stop_words(text_token):\n",
    "    return [token for token in text_token if token not in stop_words.ENGLISH_STOP_WORDS]\n",
    "\n",
    "def lem_words(text_token):\n",
    "    lem = stem.WordNetLemmatizer()\n",
    "    return [lem.lemmatize(token, pos='v') for token in text_token]\n",
    "\n",
    "def distinct_corpus_words(tokens_corpus):\n",
    "    tokens_corpus_flatten = [token for tokens in tokens_corpus for token in tokens]\n",
    "    corpus_counter = Counter(tokens_corpus_flatten).most_common()\n",
    "    return corpus_counter, len(corpus_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_single_text(text, translation_table=translation_table):\n",
    "    text = str(text).lower()\n",
    "    text = text.translate(translation_table)\n",
    "\n",
    "    text_tokens = nltk.word_tokenize(text)\n",
    "    text_tokens = remove_stop_words(text_tokens)\n",
    "    text_tokens = lem_words(text_tokens)\n",
    "    \n",
    "    return text_tokens\n",
    "\n",
    "def normalize_text(text_corpus, translation_table=translation_table):\n",
    "    return [normalize_single_text(text, translation_table=translation_table) for text in text_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(corpus_counter):\n",
    "    word2ind = {corpus_counter[i][0]: i for i in range(len(corpus_counter))}\n",
    "    ind2word = [word[0] for word in corpus_counter]\n",
    "    return word2ind, ind2word\n",
    "\n",
    "def text_token2ind(text_token, word2ind):\n",
    "    return [word2ind[token] for token in text_token if token in word2ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bow(ind_corpus, n_tokens, max_features=None):\n",
    "    \"\"\"Max features takes n features with the lowest index - assumes that lower index -> higher number of occurrences\"\"\"\n",
    "    \n",
    "    if max_features:\n",
    "        ind_corpus = [[ind for ind in text_ind if ind < max_features] for text_ind in ind_corpus]\n",
    "        n_tokens = max_features\n",
    "\n",
    "    values = []\n",
    "    col_ind = []\n",
    "    row_pointer = [0]\n",
    "\n",
    "    for features in ind_corpus:\n",
    "        feature_counter = Counter(features)\n",
    "        col_ind.extend(feature_counter.keys())\n",
    "        values.extend(feature_counter.values())\n",
    "        row_pointer.append(len(values))\n",
    "\n",
    "    S = scipy.sparse.csr_matrix((values, col_ind, row_pointer),\n",
    "                                       shape=(len(row_pointer) - 1, n_tokens),\n",
    "                                       dtype=int)\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_bow(text_ind, n_tokens, max_features=None):\n",
    "    if max_features:\n",
    "        text_ind = [ind for ind in text_ind if ind < max_features]\n",
    "        n_tokens = max_features\n",
    "\n",
    "    single_bow = np.zeros(n_tokens)\n",
    "    for ind in text_ind:\n",
    "        single_bow[ind] += 1\n",
    "    \n",
    "    return single_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_co_occurrence_matrix(ind_corpus, n_tokens, window_size=4):\n",
    "    row_ind = []\n",
    "    col_ind = []\n",
    "    values = []\n",
    "\n",
    "    for text_ind in ind_corpus:\n",
    "        for i in range(len(text_ind)):\n",
    "            for j in range(max(i-window_size, 0), min(i + window_size + 1, len(text_ind))):\n",
    "                if i != j:\n",
    "                    row_ind.extend([text_ind[i]])\n",
    "                    col_ind.extend([text_ind[j]])\n",
    "                    values.extend([1])\n",
    "    \n",
    "    S = scipy.sparse.coo_matrix((values, (row_ind, col_ind)), shape=(n_tokens, n_tokens))\n",
    "\n",
    "    return S\n",
    "\n",
    "def matrix_reduce(M, method, n_dim=2, n_iter=10):\n",
    "    \n",
    "    try:\n",
    "        if method == 'svd':\n",
    "            decomposition = TruncatedSVD(n_components=n_dim, n_iter=n_iter)\n",
    "        elif method == 'nmf':\n",
    "            decomposition = NMF(n_components=n_dim)\n",
    "        \n",
    "        M_reduced = decomposition.fit_transform(M)\n",
    "        return M_reduced\n",
    "        \n",
    "    except UnboundLocalError:\n",
    "        print('Choose either svd or nmf method')\n",
    "\n",
    "def avg_svd_embeddings(text_ind, reduced_co_occurrence_matrix, word2ind):\n",
    "    i = len(text_ind)\n",
    "\n",
    "    if i>=1:\n",
    "        return sum([reduced_co_occurrence_matrix[ind] for ind in text_ind])/i\n",
    "    return np.zeros(reduced_co_occurrence_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_w2v_embeddings(text_token, w2v_model):\n",
    "    words = [token for token in text_token if token in w2v_model.wv.vocab]\n",
    "    if len(words)>=1:\n",
    "        return np.mean(w2v_model.wv[words], axis=0)\n",
    "    return np.zeros(w2v_model.trainables.layer1_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "Creating word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>874322</th>\n",
       "      <td>EXCELLENT PRODUCT AND PRICE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381124</th>\n",
       "      <td>These are really cute.  The more you get in on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088190</th>\n",
       "      <td>Ordered for my granddaughter for christmas gif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977103</th>\n",
       "      <td>Play-doh gives hours of fun. My kids can liter...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155475</th>\n",
       "      <td>Very Pretty dice. Fun to play games with color...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    review  sentiment\n",
       "874322                         EXCELLENT PRODUCT AND PRICE          1\n",
       "381124   These are really cute.  The more you get in on...          1\n",
       "1088190  Ordered for my granddaughter for christmas gif...          1\n",
       "977103   Play-doh gives hours of fun. My kids can liter...          1\n",
       "1155475  Very Pretty dice. Fun to play games with color...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/reviews_toys_games.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.914674872933972"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['sentiment'])/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews = df['review'].to_list()\n",
    "#sentiment = df['sentiment'].to_list()\n",
    "\n",
    "reviews = df['review'][:10000].to_list()\n",
    "sentiment = df['sentiment'][:10000].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_tokens = f.normalize_text(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in the dictionary:  13656\n",
      "Most common words:  [('game', 9749), ('play', 5682), ('love', 4328), ('great', 3316), ('fun', 3309), ('kid', 2252), ('card', 2023), ('like', 2002), ('old', 1913), ('time', 1819)]\n"
     ]
    }
   ],
   "source": [
    "corpus_counter, n_tokens = f.distinct_corpus_words(reviews_tokens)\n",
    "\n",
    "print('Words in the dictionary: ', n_tokens)\n",
    "print('Most common words: ', corpus_counter[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/reviews_tokens_10k.pickle', 'wb') as handle:\n",
    "    pickle.dump(reviews_tokens, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('data/sentiment_10k.pickle', 'wb') as handle:\n",
    "    pickle.dump(sentiment, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None\n",
    "reviews = None\n",
    "reviews_tokens = None\n",
    "sentiment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/reviews_tokens_10k.pickle', 'rb') as handle:\n",
    "    reviews_tokens = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_counter, n_tokens = f.distinct_corpus_words(reviews_tokens)\n",
    "word2ind, ind2word = f.build_dictionary(corpus_counter)\n",
    "reviews_ind = [f.text_token2ind(review, word2ind) for review in reviews_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_bow = f.build_bow(reviews_ind, n_tokens, 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 6000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text of test review:  ['ok', 'really', 'consider', 'book', 'really', 'small', 'disappoint']\n",
      "occurrences of words: \n",
      "book: 1 \n",
      "really: 2\n"
     ]
    }
   ],
   "source": [
    "ind_test = 15\n",
    "print('text of test review: ', reviews_tokens[ind_test])\n",
    "print('occurrences of words:',\n",
    "      '\\nbook:', S_bow.todense()[ind_test, word2ind['book']],\n",
    "      '\\nreally:', S_bow.todense()[ind_test, word2ind['really']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/M_bow_10k.pickle', 'wb') as handle:\n",
    "    pickle.dump(S_bow, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "S_bow = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert string to BOW vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_test = 'This game is amazing ^^, my son plays with it all the time! popolsku behavoir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game', 'amaze', 'son', 'play', 'time', 'popolsku', 'behavoir']\n",
      "[0, 221, 30, 1, 9, 7999]\n"
     ]
    }
   ],
   "source": [
    "review_tokens_test = f.normalize_single_text(review_test)\n",
    "print(review_tokens_test)\n",
    "\n",
    "review_ind_test = f.text_token2ind(review_tokens_test, word2ind)\n",
    "print(review_ind_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.build_single_bow(review_ind_test, n_tokens, 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "M_tfidf = vectorizer.fit_transform([' '.join(r) for r in reviews_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 13622)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text of test review:  ['ok', 'really', 'consider', 'book', 'really', 'small', 'disappoint']\n",
      "TFIDF of words: \n",
      "book: 0.29574860929046415 \n",
      "really: 0.4896422737221947\n"
     ]
    }
   ],
   "source": [
    "ind_test = 15\n",
    "print('text of test review: ', reviews_tokens[ind_test])\n",
    "print('TFIDF of words:',\n",
    "      '\\nbook:', M_tfidf.todense()[ind_test, vectorizer.vocabulary_['book']],\n",
    "      '\\nreally:', M_tfidf.todense()[ind_test, vectorizer.vocabulary_['really']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/M_tfidf_10k.pickle', 'wb') as handle:\n",
    "    pickle.dump(M_tfidf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "M_tfidf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert test string into TFIDF vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x13622 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([' '.join(review_tokens_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD & NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_co_occurrence = f.build_co_occurrence_matrix(reviews_ind, n_tokens, window_size=4)\n",
    "svd_reduced_co_occurrence = f.matrix_reduce(S_co_occurrence, method='svd', n_dim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_svd = np.stack([f.avg_svd_embeddings(ind, svd_reduced_co_occurrence, word2ind) for ind in reviews_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.14323424e+01, -3.47109825e+00, -2.67020404e+00,\n",
       "         2.03974543e-01,  1.48255302e+01],\n",
       "       [ 2.16992669e+02,  1.05707348e+01, -1.46835127e+01,\n",
       "        -6.84907300e-01,  3.10134179e+01],\n",
       "       [ 4.55199337e+02, -3.17093833e+01, -8.54671822e+01,\n",
       "         1.60653564e+00,  2.72084063e+01],\n",
       "       ...,\n",
       "       [ 2.63608525e+03, -1.08866951e+02,  2.31153097e+02,\n",
       "        -1.73853227e+01, -1.02893709e+02],\n",
       "       [ 6.57657544e+01, -6.06839347e+00, -9.53952804e+00,\n",
       "         1.57869204e+00,  2.58041369e+01],\n",
       "       [ 1.43775318e+02,  7.27239290e+01,  2.55859105e+01,\n",
       "        -2.68445857e+00,  9.12218545e+00]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/M_svd_10k.pickle', 'wb') as handle:\n",
    "    pickle.dump(M_svd, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "M_svd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_reduced_co_occurrence = f.matrix_reduce(S_co_occurrence, method='nmf', n_dim=5)\n",
    "M_nmf = np.stack([f.avg_svd_embeddings(ind, nmf_reduced_co_occurrence, word2ind) for ind in reviews_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.18080617e-01, 7.80464327e-02, 1.49065280e-01, 9.30239777e-02,\n",
       "        5.36790040e-01],\n",
       "       [1.49840854e+00, 6.20963960e-01, 6.69917415e-01, 7.86717645e-01,\n",
       "        1.45379772e+00],\n",
       "       [4.12534986e+00, 6.89270575e-01, 1.12965690e+00, 9.85722278e-01,\n",
       "        2.46493945e+00],\n",
       "       ...,\n",
       "       [1.94969517e+01, 2.88667247e+00, 2.19527459e+01, 3.38141979e+00,\n",
       "        4.91998215e+00],\n",
       "       [3.96732276e-01, 1.17008949e-01, 1.69634013e-02, 1.07129897e-01,\n",
       "        8.91329126e-01],\n",
       "       [5.45359060e-01, 1.15693447e+00, 7.93880674e-01, 1.35079064e+00,\n",
       "        2.99975201e-01]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_nmf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/M_nmf_10k.pickle', 'wb') as handle:\n",
    "    pickle.dump(M_nmf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "M_nmf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert test string into SVD vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1774.31154901,  -57.54346441,   80.98033976,  -11.74561109,\n",
       "        -43.33517736])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.avg_svd_embeddings(review_ind_test, svd_reduced_co_occurrence, word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=5,\n",
    "                     window=3,\n",
    "                     size=100,\n",
    "                     workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(reviews_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5682288, 7245750)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(reviews_tokens, total_examples=w2v_model.corpus_count, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_word2vec = np.stack([f.avg_w2v_embeddings(review, w2v_model) for review in reviews_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_word2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/M_word2vec_10k.pickle', 'wb') as handle:\n",
    "    pickle.dump(M_word2vec, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "M_word2vec = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert test string into SVD vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4916602 , -0.673176  , -0.43748522,  0.45790607, -0.5072142 ,\n",
       "        0.25582296,  0.5711106 , -0.06258135,  0.36200437,  0.740065  ,\n",
       "        0.04109208,  0.58028495, -0.40537295,  0.6922718 ,  0.23685701,\n",
       "        0.54663134,  0.09706992, -0.14363842, -0.30137047, -0.12514031,\n",
       "        0.51345617,  0.37067157,  0.11115031,  0.04619621,  0.58541983,\n",
       "       -0.35716337, -0.43209654,  0.48820734, -0.20929281, -0.04797919,\n",
       "        0.4365302 , -0.46987385,  0.01353803, -0.04874829,  0.3549533 ,\n",
       "       -0.27242514, -0.14320196, -0.40812597, -0.19990039,  0.42948112,\n",
       "        0.5049257 , -0.10161964, -0.5768609 ,  0.39754462, -0.24381408,\n",
       "        0.33620033,  0.5084406 , -0.25106287,  0.04437397,  0.4186307 ,\n",
       "        0.7601339 , -0.5622204 , -0.32498673, -0.12162588, -0.11994795,\n",
       "       -0.12674376,  0.6258924 , -0.18641171, -0.02631643,  0.0009281 ,\n",
       "       -0.4873394 ,  0.17256328,  0.32977718,  0.12279141, -0.37370855,\n",
       "        0.07469588, -0.07714178,  0.07089421, -0.44723797,  0.02965715,\n",
       "        0.29556462, -0.20238924, -0.3977477 , -0.13225654, -0.3719462 ,\n",
       "        0.22240715,  0.58762515,  0.53940403, -0.36755988,  0.39854425,\n",
       "       -0.3566614 ,  0.5168184 , -0.12117229,  0.6296242 , -0.13860962,\n",
       "       -0.10506403,  0.23591188,  0.10076866, -0.29874626,  0.80543596,\n",
       "       -0.5370688 , -0.5854535 ,  0.02541386, -0.7247845 , -0.5576681 ,\n",
       "       -0.2465798 , -0.02603312, -0.05333928,  0.63975257, -0.15180811],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.avg_w2v_embeddings(review_tokens_test, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai-nlp",
   "language": "python",
   "name": "fastai-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
